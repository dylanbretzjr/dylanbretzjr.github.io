[
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Thank you for visiting my site :)\nFeel free to send me a message if youâ€™d like to connect!\n\n\n Back to top"
  },
  {
    "objectID": "projects/cyclistic-case-study/index.html#goal",
    "href": "projects/cyclistic-case-study/index.html#goal",
    "title": "Cyclistic Bike-Share Data Analysis with Python Case Study",
    "section": "Goal",
    "text": "Goal\nThe overall goal of the marketing analytics team is this:\n\nDesign marketing strategies aimed at converting casual riders into annual members.\n\nThree questions will guide the future marketing program:\n\nHow do annual members and casual riders use Cyclistic bikes differently?\nWhy would casual riders buy Cyclistic annual memberships?\nHow can Cyclistic use digital media to influence casual riders to become members?\n\nFor this case study, I focused on the first question: How do annual members and casual riders use Cyclistic bikes differently?"
  },
  {
    "objectID": "projects/cyclistic-case-study/index.html#business-task",
    "href": "projects/cyclistic-case-study/index.html#business-task",
    "title": "Cyclistic Bike-Share Data Analysis with Python Case Study",
    "section": "Business Task",
    "text": "Business Task\nTo address this question, I completed the following business task:\n\nAnalyze historical bike trip data to identify trends in how annual members and casual riders use Cyclistic bikes differently."
  },
  {
    "objectID": "projects/cyclistic-case-study/index.html#data-integrity",
    "href": "projects/cyclistic-case-study/index.html#data-integrity",
    "title": "Cyclistic Bike-Share Data Analysis with Python Case Study",
    "section": "Data Integrity",
    "text": "Data Integrity\nIn terms of data integrity, this dataset ROCCCs:\n\nReliableâ€”It is reasonable to assume the data is trustworthy and unbiased for the purposes of this case study.\nOriginalâ€”The data is sourced directly from the City of Chicago and Motivate International Inc., the original owners and operators (first-party source).\nComprehensiveâ€”The data contains the necessary records and fields for this analysis.\nCurrentâ€”The dataset includes historical records for several years, including the relevant records from the past 12 months.\nCitedâ€”The data is made available under a public license from the City of Chicago and Motivate International Inc."
  },
  {
    "objectID": "projects/cyclistic-case-study/index.html#tools",
    "href": "projects/cyclistic-case-study/index.html#tools",
    "title": "Cyclistic Bike-Share Data Analysis with Python Case Study",
    "section": "Tools",
    "text": "Tools\nThe combined dataset contains 5,569,279 rows, making it too large for standard spreadsheet software. (Microsoft Excel is limited to roughly 1 million rows, for example.)\nFor this project, I used Python throughout since itâ€™s super powerful and versatile and can be used end-to-end for cleaning, transformation, analysis, and visualization. Plus, Python is fun, and I wanted to practice using it for data analysis.\nI used Spyder 6 as my IDE because it includes features especially useful for scientific data analysis. My environment uses Python 3.12 with the following core libraries:\n\nnumpy and pandas for data manipulation and analysis\nmatplotlib, seaborn, and plotly for data visualization"
  },
  {
    "objectID": "projects/cyclistic-case-study/index.html#workflow",
    "href": "projects/cyclistic-case-study/index.html#workflow",
    "title": "Cyclistic Bike-Share Data Analysis with Python Case Study",
    "section": "Workflow",
    "text": "Workflow\nTo make things easier, I refactored the workflow into three scripts:\n\n_1_data_cleaning.py\n\nCombines, cleans, and transforms data for analysis, and exports it as a CSV file.\n\n_2_analysis_and_viz.py\n\nPerforms statistical analysis and generates static visualizations.\n\n_3_build_ride_map.py\n\nBuilds a dynamic map visualizing ride start and end locations by membership status.\n\n\nIn addition, the project includes a config.py file to define global file paths and a custom .mplstyle file for consistent visualization styling."
  },
  {
    "objectID": "projects/cyclistic-case-study/index.html#initial-cleaning",
    "href": "projects/cyclistic-case-study/index.html#initial-cleaning",
    "title": "Cyclistic Bike-Share Data Analysis with Python Case Study",
    "section": "Initial Cleaning",
    "text": "Initial Cleaning\nWith the data loaded and combined, I proceeded to perform some basic data exploration and cleaning.\n\nCheck Duplicates\nFirst, I checked for duplicate rows and rides, since duplicates could bias the analysis.\ndf.duplicated().sum()\ndf['ride_id'].duplicated().sum()\nNo duplicates were found, so no action was required.\n\n\nDrop Station ID Fields\nNext, I dropped the start_station_id and end_station_id columns.\nThese columns contained inconsistencies due to station ID changes made between May and June 2025. Since station IDs were not required for this analysis anyway (station names and coordinates were sufficient), I dropped both columns to reduce noise and avoid potential confusion later on.\ndf = df.drop(columns=['start_station_id', 'end_station_id'])\n\n\nConvert Timestamps to Datetime\nFinally, I converted the started_at and ended_at columns to pandas datetime objects so I could transform the data using timestamps later in the process.\ndf['started_at'] = pd.to_datetime(df['started_at'])\ndf['ended_at'] = pd.to_datetime(df['ended_at'])"
  },
  {
    "objectID": "projects/cyclistic-case-study/index.html#handling-missing-values",
    "href": "projects/cyclistic-case-study/index.html#handling-missing-values",
    "title": "Cyclistic Bike-Share Data Analysis with Python Case Study",
    "section": "Handling Missing Values",
    "text": "Handling Missing Values\nNext, I performed a null count and found 2,397,565 missing values overall (i.e., empty cells across all columns).\ndf.isna().sum().sum()\nTo understand more, I also performed a null count by column.\ndf.isna().sum()\n\n\n\nColumn\nNull Count\n\n\n\n\nstart_station_name\n1,166,861\n\n\nend_station_name\n1,219,784\n\n\nend_lat\n5,460\n\n\nend_lng\n5,460\n\n\n\n\nDrop Rides Missing End Coordinates\nSince both the end_lat and end_lng columns were missing the exact same number of missing values, I checked whether both values were missing in the same rows by counting the rows missing both end coordinates.\ndf[[\"end_lat\", \"end_lng\"]].isna().all(axis=1).sum()\n5,460 rows were missing values in both the end_lat and end_lng columns, confirming that the missing coordinate data belonged to the same rows.\nThese rows comprised only 0.1% of the dataset, so dropping them wonâ€™t have a significant impact on the analysis. Therefore, I dropped them in order to be able to use the remaining coordinates for mapping.\n\n\nFill Missing Station Names\nNext, I addressed the missing start and end station names.\nSince so many rows were missing station names, dropping them would likely introduce bias and compromise the analysis.\nInstead, I filled the missing station names with a unique placeholder value (no_station_recorded). This approach preserved ride records while still allowing for aggregate analysis using station names.\ndf['start_station_name'] = (\n    df['start_station_name'].fillna('no_station_recorded')\n)\ndf['end_station_name'] = (\n    df['end_station_name'].fillna('no_station_recorded')\n)"
  },
  {
    "objectID": "projects/cyclistic-case-study/index.html#validate-timeframe",
    "href": "projects/cyclistic-case-study/index.html#validate-timeframe",
    "title": "Cyclistic Bike-Share Data Analysis with Python Case Study",
    "section": "Validate Timeframe",
    "text": "Validate Timeframe\nBefore transforming the data further, I checked whether all trips fell within the intended analysis window (2024-11-01 to 2025-10-31).\nI first checked the minimum and maximum trip start and end timestamps.\ndf[\"started_at\"].min()\ndf[\"started_at\"].max()\ndf[\"ended_at\"].min()\ndf[\"ended_at\"].max()\nThe earliest start date was 2024-10-31, which falls one day before the analysis window.\nI then counted how many rides started before the analysis window.\nlen(df[df['started_at'] &lt; TIMEFRAME_START])\nThere were 33 rides that started before 2024-11-01. These rides all ended after the window started, which explains why they were included in the dataset in the first place.\nTo keep the dataset consistent and avoid their impact on my analysis and visualizations, I dropped these 33 rows."
  },
  {
    "objectID": "projects/cyclistic-case-study/index.html#drop-rides-affected-by-dst",
    "href": "projects/cyclistic-case-study/index.html#drop-rides-affected-by-dst",
    "title": "Cyclistic Bike-Share Data Analysis with Python Case Study",
    "section": "Drop Rides Affected by DST",
    "text": "Drop Rides Affected by DST\nTo validate ride duration values, I first checked for negative-duration rides.\nlen(df[df[\"ride_duration_min\"] &lt; 0])\nThere were 43 rides with negative durations. After investigating these records, I found that they allâ€¦\n\noccurred on the same day (2024-11-03),\nstarted at the same time (between 1 a.m. and 2 a.m.),\nand ended at the same time (between 1 a.m. and 2 a.m.).\n\nThis led me to suspect that the negative-duration rides could have been caused by daylight saving time (DST). I confirmed that Chicago observed DST and that the clocks turned back 1 hour at 2 a.m. on 2024-11-03, which coincided with the negative-duration rides in the dataset.\nWhile there were only 43 rides with negative durations, I couldnâ€™t simply drop them since there were likely other rows affected by the DST time shift.\nBasically, if a ride occurred during the fall time shift window, the calculated ride duration would be 1 hour less than it actually was, causing rides shorter than 1 hour to be negative. And if a ride occurred during the spring time shift window, the calculated ride duration would be 1 hour more than it actually was.\nIn total, 507 rows (0.01%) were affected by DSTâ€”476 in the fall and 31 in the spring. Since so few rows were affected, I dropped them with negligible impact on the findings.\nWhile it also would have been possible to adjust the timestamps and correct these rows, dropping them was the simplest and cleanest approach for this case study, especially considering the negligible loss of data."
  },
  {
    "objectID": "projects/cyclistic-case-study/index.html#drop-ride-duration-outliers",
    "href": "projects/cyclistic-case-study/index.html#drop-ride-duration-outliers",
    "title": "Cyclistic Bike-Share Data Analysis with Python Case Study",
    "section": "Drop Ride Duration Outliers",
    "text": "Drop Ride Duration Outliers\nThe final cleaning step was to inspect the distribution of ride durations for extreme outliers that might not represent real rider behavior.\nFirst, I checked the minimum and maximum ride duration.\ndf['ride_duration_min'].min()\ndf['ride_duration_min'].max()\nThe minimum ride duration was 0.0 minutes, while the maximum ride duration exceeded a day (1499.97 minutes). Neither extreme would likely represent valid trips.\nTo better understand this, I plotted the distribution of ride lengths.\nFull Distribution of Ride Duration:\n\nThe vast majority of rides lasted 1 hour or less.\nHowever, the distribution showed a long right tail of unusually lengthy rides. To make this tail more apparent, I plotted ride frequency on a logarithmic scale.\nFull Distribution of Ride Duration (Log Scale):\n\nOverall, the distribution is heavily right-skewed, with most rides concentrated at shorter durations.\nTo take a closer look at this concentration, I zoomed in on rides under 1 hour.\nDistribution of Ride Duration Under 1 Hour:\n\nAt this scale, there is a noticeable spike in very short rides, with an unusually high count lasting 1 minute or less.\nTo examine this more closely, I zoomed in further to rides under 5 minutes.\nDistribution of Ride Duration Under 5 Minutes:\n\nThis revealed an especially high concentration of rides lasting 30 seconds or less.\nI suspected that the unusually high frequency of extremely short rides may have been caused by false starts (undocking and quickly re-docking a bike in favor of another) or canceled rides. Extremely long rides, on the other hand, may have been caused by docking errors or forgotten rides.\nSince both extremely short and extremely long rides likely do not represent typical rider behavior, I dropped all rides lasting 1 minute or less and all rides lasting 1 day or more.\n142,855 rides lasting 1 minute or less and 207 rides lasting 1 day or more were removed for a total of 143,062 rows, representing 2.57% of the original combined dataset."
  },
  {
    "objectID": "projects/cyclistic-case-study/index.html#cleaning-summary",
    "href": "projects/cyclistic-case-study/index.html#cleaning-summary",
    "title": "Cyclistic Bike-Share Data Analysis with Python Case Study",
    "section": "Cleaning Summary",
    "text": "Cleaning Summary\nTo review, here is a summary of all the steps so far:\n\nLoaded and combined data\n\nTotal records: 5,569,279\n\nChecked for duplicate rows/rides: 0 duplicates\nDropped start_station_id and end_station_id columns\nConverted started_at and ended_at columns to datetime\nChecked for null values: 2,397,565 nulls\nDropped rows missing end coordinates: 5,460 (0.10%)\nFilled missing station names with a placeholder (no_station_recorded)\nDropped rides starting before timeframe: 33 (0.00%)\nTransformed data:\n\nExtracted start date, month, week, day of week, and hour to new columns\nCalculated ride duration in minutes in a new column\n\nDropped rows affected by DST: 507 (0.01%)\nDropped rides 1 minute or less and rides 1 day or more: 143,062 (2.57%)\n\nIn total, 149,062 rows (2.68%) were dropped during cleaning, leaving 5,420,217 rows (over 97%) for analysis."
  },
  {
    "objectID": "projects/cyclistic-case-study/index.html#export-cleaned-dataset",
    "href": "projects/cyclistic-case-study/index.html#export-cleaned-dataset",
    "title": "Cyclistic Bike-Share Data Analysis with Python Case Study",
    "section": "Export Cleaned Dataset",
    "text": "Export Cleaned Dataset\nWith the dataset cleaned and validated, I exported and saved it as a CSV file. This way, Iâ€™ll have a copy of the cleaned data and wonâ€™t have to run the cleaning script every time.\nBefore exporting, I reset the index to ensure row numbers were continuous after manipulating the data.\ndf = df.reset_index(drop=True)\n\ndf.to_csv(config.CLEAN_PATH, index=False)\nSince the dataset is large, I validated the export by re-importing the cleaned CSV and confirming that it matched the dataframe used to create it.\nclean_df = pd.read_csv(\n    config.CLEAN_PATH, parse_dates=['started_at', 'ended_at']\n)\n\ntry:\n    pd.testing.assert_frame_equal(\n        df, clean_df, check_dtype=False, check_exact=False\n    )\n    print('SUCCESS: Dataframes match.')\n    del clean_df\nexcept AssertionError as e:\n    print('WARNING: Dataframes do not match:')\n    print(e)\nThe datasets matched, confirming that the cleaned dataset exported successfully."
  },
  {
    "objectID": "projects/cyclistic-case-study/index.html#summary-tables",
    "href": "projects/cyclistic-case-study/index.html#summary-tables",
    "title": "Cyclistic Bike-Share Data Analysis with Python Case Study",
    "section": "Summary Tables",
    "text": "Summary Tables\nThe first step of the analysis was to create summary tables for each statistic.\nTo start, I created summary tables to calculate the total counts and proportion of rides by membership for the following:\n\nOverallâ€”the entire 12-month period (2024-11-01 to 2025-10-31)\nMonthâ€”per month\nWeekâ€”per week\nWeekdayâ€”per day of week (Sundayâ€“Saturday)\nWeekendâ€”weekday vs weekend\nHourâ€”per hour of day\nBike typeâ€”by bike type (â€˜classicâ€™ or â€˜electricâ€™)\nRound-tripâ€”one-way vs round-trip\n\nsum_overall = get_counts_and_pct(df, ['member_casual'])\n\nsum_month = get_counts_and_pct(\n    df, ['start_month', 'member_casual']\n)\n\nsum_week = get_counts_and_pct(\n    df, ['start_week', 'member_casual']\n)\n\nsum_weekday = get_counts_and_pct(\n    df, ['start_weekday', 'member_casual']\n)\n\nsum_weekend = get_counts_and_pct(\n    df.assign(\n        day_type=np.where(\n            df['start_weekday'].isin([0, 6]), 'Weekend', 'Weekday'\n            )\n    ),\n    ['day_type', 'member_casual']\n)\n\nsum_hour = get_counts_and_pct(\n    df, ['start_hour', 'member_casual']\n)\n\nsum_biketype = get_counts_and_pct(\n    df, ['rideable_type', 'member_casual']\n)\n\nsum_roundtrip = get_counts_and_pct(\n    df.assign(\n        is_roundtrip=df['start_station_name'] == df['end_station_name']\n    ),\n    ['is_roundtrip', 'member_casual']\n)\nIn order to visualize the percent change per month by membership, I calculated this in a new column in the monthly counts and percentages summary table (sum_month).\nsum_month['pct_change'] = (\n    sum_month\n    .groupby('member_casual')['count']\n    .pct_change() * 100\n)\nNext, I calculated ride duration statistics (min, max, mean, median, mode, Q1, Q3, IQR) by membership status for the following:\n\nOverallâ€”the entire 12-month period (2024-11-01 to 2025-10-31)\nMonthâ€”per month\nWeekdayâ€”per day of week (Sundayâ€“Saturday)\nBike typeâ€”by bike type (â€˜classicâ€™ or â€˜electricâ€™)\n\nstats_aggs = {\n    'min': 'min',\n    'max': 'max',\n    'mean': 'mean',\n    'median': 'median',\n    'mode': lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan,\n    'q1': lambda x: x.quantile(0.25),\n    'q3': lambda x: x.quantile(0.75),\n    'iqr': lambda x: x.quantile(0.75) - x.quantile(0.25)\n}\n\nduration_overall = (\n    df.groupby('member_casual')['ride_duration_min']\n    .agg(**stats_aggs).reset_index()\n)\n\nduration_month = (\n    df.groupby(['start_month', 'member_casual'])['ride_duration_min']\n    .agg(**stats_aggs).reset_index()\n)\n\nduration_weekday = (\n    df.groupby(['start_weekday', 'member_casual'])['ride_duration_min']\n    .agg(**stats_aggs).reset_index()\n)\n\nduration_biketype = (\n    df.groupby(['rideable_type', 'member_casual'])['ride_duration_min']\n    .agg(**stats_aggs).reset_index()\n)\nI also found the mode start day of week (most popular day) by membership status:\nmode_weekday = (\n    df.pivot_table(\n        index='member_casual',\n        values='start_weekday',\n        aggfunc=lambda x: x.mode().iloc[0] if not x.mode().empty else None,\n        margins=True,\n        margins_name='overall'\n    )\n    .rename(columns={'start_weekday': 'mode_weekday'})\n    .reset_index()\n)\nThen, I found the top stations, routes (station-to-station pairs), and round-trips (same start and end station) for both members and casual riders.\n# Top stations\ntop_start_m = get_ranking(df, ['start_station_name'], 'member')\ntop_start_c = get_ranking(df, ['start_station_name'], 'casual')\n\ntop_end_m = get_ranking(df, ['end_station_name'], 'member')\ntop_end_c = get_ranking(df, ['end_station_name'], 'casual')\n\n# Top routes\ntop_routes_m = get_ranking(\n    df, ['start_station_name', 'end_station_name'], 'member'\n)\ntop_routes_c = get_ranking(\n    df, ['start_station_name', 'end_station_name'], 'casual'\n)\n\n# Top round-trips\nis_roundtrip = df['start_station_name'] == df['end_station_name']\n\ntop_roundtrips_m = get_ranking(\n    df[is_roundtrip], ['start_station_name'], 'member'\n)\ntop_roundtrips_c = get_ranking(\n    df[is_roundtrip], ['start_station_name'], 'casual'\n)\nFinally, I calculated the ride density by membership for each hour of each day of the week (Sundayâ€“Saturday) to create the heat maps.\ndensity_day_hour_m = get_ranking(\n    df, ['start_weekday', 'start_hour'], 'member'\n)\n\ndensity_day_hour_c = get_ranking(\n    df, ['start_weekday', 'start_hour'], 'casual'\n)"
  },
  {
    "objectID": "projects/cyclistic-case-study/index.html#pivot-tables",
    "href": "projects/cyclistic-case-study/index.html#pivot-tables",
    "title": "Cyclistic Bike-Share Data Analysis with Python Case Study",
    "section": "Pivot Tables",
    "text": "Pivot Tables\nTo prepare the data for plotting, I created pivot tables from the summary tables created in the previous section.\npivot_monthly_counts = sum_month.pivot(\n    index='start_month',\n    columns='member_casual',\n    values='count'\n)\n\npivot_weekly_counts = (\n    sum_week\n        .pivot(index='start_week', columns='member_casual', values='count')\n        .fillna(0)\n        .sort_index()\n)\n\npivot_monthly_pct_change = (\n    sum_month\n    .pivot(\n        index='start_month',\n        columns='member_casual',\n        values='pct_change'\n    )\n    .sort_index()\n)\n\npivot_weekday_counts = sum_weekday.pivot(\n    index='start_weekday',\n    columns='member_casual',\n    values='count'\n)\n\npivot_weekday_prop = (\n    sum_weekday\n    .pivot(index='start_weekday', columns='member_casual', values='percentage')\n    .reindex(range(7), fill_value=0)\n    .sort_index()\n)\n\npivot_weekend_counts = (\n    sum_weekend\n    .pivot(index='day_type', columns='member_casual', values='count')\n    .reindex(['Weekday', 'Weekend'])\n)\n\npivot_weekend_prop = (\n    sum_weekend\n    .pivot(index='day_type', columns='member_casual', values='percentage')\n    .reindex(['Weekday', 'Weekend'])\n)\n\npivot_hour_counts = (\n    sum_hour\n        .pivot(index='start_hour', columns='member_casual', values='count')\n        .fillna(0)\n        .sort_index()\n)\n\npivot_density_day_hour_m = (\n    density_day_hour_m\n        .pivot(index='start_weekday', columns='start_hour', values='count')\n        .reindex(index=range(7), columns=range(24), fill_value=0)\n)\n\npivot_density_day_hour_c = (\n    density_day_hour_c\n        .pivot(index='start_weekday', columns='start_hour', values='count')\n        .reindex(index=range(7), columns=range(24), fill_value=0)\n)\n\npivot_density_day_hour_m = pivot_density_day_hour_m.div(\n    pivot_density_day_hour_m.sum(axis=1), axis=0\n)\npivot_density_day_hour_c = pivot_density_day_hour_c.div(\n    pivot_density_day_hour_c.sum(axis=1), axis=0\n)\n\npivot_duration_weekday = duration_weekday.pivot(\n    index='start_weekday',\n    columns='member_casual',\n    values='median'\n)\n\npivot_roundtrip_counts = sum_roundtrip.pivot(\n    index='is_roundtrip',\n    columns='member_casual',\n    values='count'\n)\n\npivot_roundtrip_prop = (\n    sum_roundtrip\n    .pivot(index='is_roundtrip', columns='member_casual', values='percentage')\n    .reindex([0, 1])\n)\n\npivot_biketype_counts = sum_biketype.pivot(\n    index='rideable_type',\n    columns='member_casual',\n    values='count'\n)\n\npivot_biketype_prop = (\n    sum_biketype\n    .pivot(index='rideable_type', columns='member_casual', values='percentage')\n)\nNote: No pivot table was needed for sum_overall since the data was ready to be plotted as-is.\nIn addition, I prepared the data for the box plot showing ride duration distribution by membership status.\ndist_duration_overall = (\n    df[['ride_duration_min', 'member_casual']]\n    .assign(membership_group=lambda d: d['member_casual'].map({\n        'member': 'Member',\n        'casual': 'Casual'\n    }))\n    [['ride_duration_min', 'membership_group']]\n)\n\nmedian_values = (\n    dist_duration_overall\n    .groupby('membership_group')['ride_duration_min']\n    .median()\n)"
  },
  {
    "objectID": "projects/cyclistic-case-study/index.html#key-insights",
    "href": "projects/cyclistic-case-study/index.html#key-insights",
    "title": "Cyclistic Bike-Share Data Analysis with Python Case Study",
    "section": "Key Insights",
    "text": "Key Insights\nThe visualizations below summarize the key insights on how members and casual riders use bikes differently.\n\nOverall\nTotal Rides by Membership Status:\n\n\nAnnual members make up the majority of rides overall (65%)\nCasual riders still make up a significant portion (35%)\n\n\n\nSeasonality\nTotal Monthly Rides by Membership Status:\n\n\nRides peaked during warmer months (summer and early fall)\n\nWeekly Proportion of Rides by Membership Status:\n\n\nProportion of casual riders increases during peak riding season\n\nMonthly Percent Change in Rides by Membership Status:\n\n\nOverall positive growth from February through August, with most growth during spring\nHuge spike of casual riders in March (+207% from February)\n\n\n\nDay of Week\nTotal Daily Rides by Membership Status:\n\n\nTotal member rides are higher on weekdays (â€˜nâ€™ shaped)\nThe busiest day for members is Thursday\nTotal casual rides are higher on weekends (â€˜uâ€™ shaped)\nThe busiest day for casual riders is Saturday\n\nProportion of Weekday vs Weekend Rides by Membership Status:\n\n\nProportion of casual riders is higher on weekends\n\n\n\nTime of Day\nTotal Rides per Start Hour by Membership Status:\n\n\nTwo member daily commute spikes at 8 a.m. and 5 p.m.\nMember lunch rush around noon\nCasual rides gradually build throughout the day (until 5 p.m.)\n\nRide Density per Day and Hour by Membership Status:\n\nRide Density per Day and Hour (Members):\n\n\nDistinct â€˜Oâ€™ shape\nMember activity corresponds with weekday commute times and weekend afternoons\n\nRide Density per Day and Hour (Casual Riders):\n\n\nFaint â€˜Oâ€™ shape\nCasual rider activity is more dispersed across time and day\n\n\n\nRide Duration\nDistribution of Ride Durations by Membership Status:\n\n\nCasual riders tend to take slightly longer rides\nMember median ride length is 8.7 minutes\nCasual rider median ride length is 11.9 minutes\n\nMedian Ride Duration per Weekday by Membership Status:\n\n\nMember median ride duration is fairly consistent day-to-day\nCasual riders take longer rides on weekends\n\n\n\nLocation\nTop 10 Start Stations by Membership Status:\n\nTop 10 End Stations by Membership Status:\n\n\nNo overlap between top 10 start or end stations for members and casual riders\nTop start and end stations are consistent\n\n\n\nRound Trips\nTotal One-Way vs Round-Trips by Membership Status:\n\n\nMost trips are one-way (different start and end stations)\n\nProportion of One-Way vs Round-Trips by Membership Status:\n\n\nLarger proportion of casual riders take round trips"
  },
  {
    "objectID": "projects/cyclistic-case-study/index.html#additional-visuals",
    "href": "projects/cyclistic-case-study/index.html#additional-visuals",
    "title": "Cyclistic Bike-Share Data Analysis with Python Case Study",
    "section": "Additional Visuals",
    "text": "Additional Visuals\nThe following visualizations provide additional context and are included for reference.\nDaily Proportion of Rides by Membership Status:\n\nTotal Weekday vs Weekend Rides by Membership Status:\n\nHourly Proportion of Rides by Membership Status:\n\nTotal Classic vs Electric Bike Rides by Membership Status:\n\nProportion of Classic vs Electric Bike Rides by Membership Status:"
  },
  {
    "objectID": "projects/cyclistic-case-study/index.html#location-map",
    "href": "projects/cyclistic-case-study/index.html#location-map",
    "title": "Cyclistic Bike-Share Data Analysis with Python Case Study",
    "section": "Location Map",
    "text": "Location Map\nTo better compare geographic differences, I created a dynamic, density-weighted point map showing ride start and end locations by membership type using Plotly. I exported it to HTML with custom JavaScript that links the pan and zoom of the maps and adds responsive scaling for different screen sizes.\nRide Locations by Membership Status:\n\n\nMember ride locations are more dispersed, while casual riders tend to cluster near the lakeshore\n\nThe map can be accessed here.\nBecause the map is interactive, you can pan and zoom around to view specific areas."
  },
  {
    "objectID": "projects/cyclistic-case-study/index.html#seasonality-1",
    "href": "projects/cyclistic-case-study/index.html#seasonality-1",
    "title": "Cyclistic Bike-Share Data Analysis with Python Case Study",
    "section": "1. Seasonality",
    "text": "1. Seasonality\nInsights:\n\nPeak riding season during summer and early fall\nHigher proportion of casual riders during peak riding season\nHuge spike of casual riders in spring\n\nRecommendation:\n\nIncrease marketing efforts in spring and summer\nOffer seasonal memberships to attract casual riders"
  },
  {
    "objectID": "projects/cyclistic-case-study/index.html#time-and-day",
    "href": "projects/cyclistic-case-study/index.html#time-and-day",
    "title": "Cyclistic Bike-Share Data Analysis with Python Case Study",
    "section": "2. Time and Day",
    "text": "2. Time and Day\nInsights:\n\nHigher proportion of casual riders on weekends\nCasual rider activity is more dispersed across time and day\n\nRecommendation:\n\nOffer weekend-only memberships to attract casual weekend-only riders\nOffer optional packages during slow times/days"
  },
  {
    "objectID": "projects/cyclistic-case-study/index.html#location-1",
    "href": "projects/cyclistic-case-study/index.html#location-1",
    "title": "Cyclistic Bike-Share Data Analysis with Python Case Study",
    "section": "3. Location",
    "text": "3. Location\nInsights:\n\nTop start and end stations are consistent\nCasual riders tend to cluster near the lakeshore\n\nRecommendation:\n\nTarget areas around the lakeshore and top stations for casual riders"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "A collection of completed projects and case studies.\n\n\n\n\n\n\n\n\n\n\n\nMinecraft Villager Trading Database: System Design and Implementation\n\n\n\nminecraft\n\ndata engineering\n\ndatabase normalization\n\nrdbms\n\nsql\n\nsqlite\n\npython\n\netl\n\n\n\n\n\n\n\n\n\n2026-02-20\n\n\nDylan Bretz Jr.\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\nCyclistic Bike-Share Data Analysis with Python Case Study\n\n\n\ncase study\n\ndata analysis\n\ndata visualization\n\npython\n\netl\n\n\n\n\n\n\n\n\n\n2026-01-19\n\n\nDylan Bretz Jr.\n\n24 min\n\n\n\n\nNo matching items\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "projects/minecraft-trading-database-implementation/index.html#villagers-and-trading-mechanics",
    "href": "projects/minecraft-trading-database-implementation/index.html#villagers-and-trading-mechanics",
    "title": "Minecraft Villager Trading Database: System Design and Implementation",
    "section": "Villagers and Trading Mechanics",
    "text": "Villagers and Trading Mechanics\nVillagers are non-playable characters that live in villages and can be assigned different professions, such as armorer, farmer, or librarian. Players can trade with villagers that have a profession using emeralds as currency to buy and sell items.\nEach villager starts out with a limited number of trades, and additional trades can be unlocked through repeated trading. Trades are randomly selected from a set of possible offers specific to each profession."
  },
  {
    "objectID": "projects/minecraft-trading-database-implementation/index.html#librarian-villagers",
    "href": "projects/minecraft-trading-database-implementation/index.html#librarian-villagers",
    "title": "Minecraft Villager Trading Database: System Design and Implementation",
    "section": "Librarian Villagers",
    "text": "Librarian Villagers\nOne of the most valuable professions is the librarian. Not only can you easily get lots of emeralds by selling them paper, but you can also buy enchanted books, which are used to enhance armor, tools, and weapons. Having a good supply of emeralds and enchanted books can be a real game changer, which makes trading with librarians very useful.\nHowever, finding a librarian with a specific enchantment can be tedious (if you know, you know).\nCurrently, librarian enchanted book trades are randomly selected from a set of 40 enchantments, and each librarian can offer up to 4 enchanted book trades. In addition, both the enchantment level and the cost in emeralds are randomly determined.\nAs a result, obtaining high-level enchantments at reasonable prices often requires cycling through and retaining many librarians. While desirable trades can be locked in once found, managing multiple librarians and their offers quickly becomes difficult as their numbers grow."
  },
  {
    "objectID": "projects/minecraft-trading-database-implementation/index.html#villager-trading-halls",
    "href": "projects/minecraft-trading-database-implementation/index.html#villager-trading-halls",
    "title": "Minecraft Villager Trading Database: System Design and Implementation",
    "section": "Villager Trading Halls",
    "text": "Villager Trading Halls\nTo manage large numbers of villagers, players often build villager trading halls, which are dedicated structures designed to house and organize many villagers in a centralized location.\n\nA trading hall may contain dozens of villagers across multiple professions, each offering multiple trades. While the physical organization improves accessibility, it does not address the underlying information management challenge.\nPlayers still need to rememberâ€¦\n\nwhich villager offers which trade,\nwhere that villager is located,\nand whether a better trade exists elsewhere.\n\nAs the number of villagers increases, manually tracking this information becomes impractical. The operational challenges of high volume, random variability, and frequent cross-referencing motivated the need for a structured, queryable database system."
  },
  {
    "objectID": "projects/minecraft-trading-database-implementation/index.html#villager-trade-rebalance",
    "href": "projects/minecraft-trading-database-implementation/index.html#villager-trade-rebalance",
    "title": "Minecraft Villager Trading Database: System Design and Implementation",
    "section": "Villager Trade Rebalance",
    "text": "Villager Trade Rebalance\nIt should be noted that this project is designed around standard Minecraft mechanics. The Villager Trade Rebalance is an optional experimental feature that significantly alters the villager trading mechanics. Since this feature is disabled in my world, it falls outside the scope of this project. Supporting this feature would require substantial changes to the database schema and logic."
  },
  {
    "objectID": "projects/minecraft-trading-database-implementation/index.html#spreadsheet-limitations",
    "href": "projects/minecraft-trading-database-implementation/index.html#spreadsheet-limitations",
    "title": "Minecraft Villager Trading Database: System Design and Implementation",
    "section": "Spreadsheet Limitations",
    "text": "Spreadsheet Limitations\nInitially, I tracked librarian trades in a spreadsheet. While manageable at small scales, this approach became increasingly difficult to maintain as the number of villagers and trades grew.\nThe workflow required frequent cross-referencing to answer questions, such as which enchantments were missing, where a specific trade was located, or whether a better offer existed. Because related information was stored across multiple sheets, this process involved manual filtering and comparison.\nIn addition, repeated text values, such as enchantment names, villager IDs, and locations, increased the risk of inconsistencies and data entry errors. As the dataset expanded, the spreadsheet also became slower to navigate and more difficult to maintain.\nTo address these limitations, I implemented a relational database management system (RDBMS), creating a structured and scalable solution for managing the data and supporting ongoing analysis."
  },
  {
    "objectID": "projects/minecraft-trading-database-implementation/index.html#database-normalization",
    "href": "projects/minecraft-trading-database-implementation/index.html#database-normalization",
    "title": "Minecraft Villager Trading Database: System Design and Implementation",
    "section": "Database Normalization",
    "text": "Database Normalization\nThe database was designed using normalization principles to reduce redundancy and improve data integrity.\nRather than storing all information in a single table, core entities, such as enchantments, professions, locations, and villagers, are separated into dedicated tables. Relationships between these entities are maintained using foreign keys, allowing trade records to reference standardized values instead of repeating text fields.\nThis structure supports one-to-many relationships (for example, a location can contain many villagers, and a villager can offer multiple trades) while ensuring that updates to reference data only need to be made in one place. The result is a more consistent, maintainable dataset that supports efficient querying."
  },
  {
    "objectID": "projects/minecraft-trading-database-implementation/index.html#database-schema",
    "href": "projects/minecraft-trading-database-implementation/index.html#database-schema",
    "title": "Minecraft Villager Trading Database: System Design and Implementation",
    "section": "Database Schema",
    "text": "Database Schema\nThe schema separates related data into distinct tables.\n\n\n\n\n\n\n\nTable\nDescription\n\n\n\n\nenchantments\nContains all tradeable enchantments, including their maximum level and supported item types\n\n\njobs\nContains all possible villager professions in the game\n\n\nlocations\nStores trading hall names and their coordinates\n\n\nvillagers\nStores unique villager IDs and links each to a location and profession\n\n\nlibrarian_trades\nStores individual enchanted book offers for each librarian, including the enchantment level and cost\n\n\n\nThe enchantments and jobs tables are populated from Minecraftâ€™s source files through the ETL process, while the remaining tables store data that is manually entered for new locations, villagers, and trade offers.\nThis structure supports efficient analysis and can be extended to additional villager professions as the system grows."
  },
  {
    "objectID": "projects/minecraft-trading-database-implementation/index.html#data-source",
    "href": "projects/minecraft-trading-database-implementation/index.html#data-source",
    "title": "Minecraft Villager Trading Database: System Design and Implementation",
    "section": "Data Source",
    "text": "Data Source\nMinecraft release metadata is accessible through a public version manifest, which provides information for each release. Each release entry links to version-specific metadata, including the download location for the corresponding client.jar archive. This archive contains structured data that define core gameplay elements, such as enchantments and villager professions.\nSince these files are distributed and maintained directly by the game developer, they provide an accurate and reliable source of reference data."
  },
  {
    "objectID": "projects/minecraft-trading-database-implementation/index.html#extraction-transformation-and-loading",
    "href": "projects/minecraft-trading-database-implementation/index.html#extraction-transformation-and-loading",
    "title": "Minecraft Villager Trading Database: System Design and Implementation",
    "section": "Extraction, Transformation, and Loading",
    "text": "Extraction, Transformation, and Loading\nThe ETL (extract, transform, load) process begins by retrieving the version manifest and identifying the most recent stable release. The corresponding client.jar archive is then downloaded, and its contents are extracted.\nInformation about enchantments and villager professions is stored across multiple JSON files within the archive. These files are then located and parsed to extract the relevant reference data.\nFor enchantments, the data is filtered to include only those that can be obtained through villager trading, and the enchantment, its maximum level, and the supported item type are loaded into the enchantments table. Similarly, the full list of villager professions is loaded into the jobs table.\nSince this process is automated, the reference data can be updated with new releases, ensuring the database remains current."
  },
  {
    "objectID": "projects/minecraft-trading-database-implementation/index.html#database-initialization",
    "href": "projects/minecraft-trading-database-implementation/index.html#database-initialization",
    "title": "Minecraft Villager Trading Database: System Design and Implementation",
    "section": "Database Initialization",
    "text": "Database Initialization\nAfter loading the reference data, I created the remaining tables using SQL to store information about trading hall locations, villagers, and their trade offers.\nThe locations table stores the names of trading halls and their coordinates:\nCREATE TABLE IF NOT EXISTS locations (\n    location TEXT PRIMARY KEY,\n    x_coord INTEGER,\n    z_coord INTEGER\n);\nThe villagers table records individual villagers and links each one to a location and profession:\nCREATE TABLE IF NOT EXISTS villagers (\n    villager_id TEXT PRIMARY KEY,\n    location TEXT,\n    job TEXT,\n\n    FOREIGN KEY(location) REFERENCES locations(location),\n    FOREIGN KEY(job) REFERENCES jobs(job)\n);\nThe librarian_trades table records individual enchanted book offers, linking a villager to an enchantment and storing the associated level and emerald cost:\nCREATE TABLE IF NOT EXISTS librarian_trades (\n    trade_id INTEGER PRIMARY KEY AUTOINCREMENT,\n    villager_id TEXT,\n    enchantment TEXT,\n    enchantment_level INTEGER,\n    cost_emeralds INTEGER,\n\n    FOREIGN KEY(villager_id) REFERENCES villagers(villager_id),\n    FOREIGN KEY(enchantment) REFERENCES enchantments(enchantment)\n);\nForeign key constraints enforce referential integrity by ensuring that villagers reference valid locations and professions and that trade records reference valid villagers and enchantments."
  },
  {
    "objectID": "projects/minecraft-trading-database-implementation/index.html#manual-data-entry-sql",
    "href": "projects/minecraft-trading-database-implementation/index.html#manual-data-entry-sql",
    "title": "Minecraft Villager Trading Database: System Design and Implementation",
    "section": "Manual Data Entry (SQL)",
    "text": "Manual Data Entry (SQL)\nNew records can be entered directly using standard SQL statements.\nAdd a new trading hall with location coordinates:\nINSERT INTO locations\nVALUES\n    ('spawn', 250, -45);\nAdd new villagers, their trading halls, and their professions:\nINSERT INTO villagers\nVALUES\n    ('spa001', 'spawn', 'librarian'),\n    ('spa002', 'spawn', 'librarian'),\n    ('spa003', 'spawn', 'librarian'),[...];\nAdd new librarian enchanted book trades, including enchantment level and cost:\nINSERT INTO librarian_trades (\n    villager_id,\n    enchantment,\n    enchantment_level,\n    cost_emeralds\n)\nVALUES\n    ('spa001', 'thorns', 3, 24),\n    ('spa001', 'quick_charge', 3, 38),\n    ('spa001', 'looting', 1, 5),[...];\nWhile this approach provides full control, it is inefficient for routine use. Entering multiple trades requires repeated statements, manual validation, and careful attention to formatting and constraints. As the dataset grows, the process becomes time-consuming and increases the risk of entry errors."
  },
  {
    "objectID": "projects/minecraft-trading-database-implementation/index.html#cli-based-data-entry",
    "href": "projects/minecraft-trading-database-implementation/index.html#cli-based-data-entry",
    "title": "Minecraft Villager Trading Database: System Design and Implementation",
    "section": "CLI-Based Data Entry",
    "text": "CLI-Based Data Entry\nTo support ongoing use, I developed an interactive Python-based CLI to streamline and standardize librarian trade entry.\nThe CLI provides a structured workflow that reduces repetitive input by retaining the current location or villager context when multiple trades are entered. Validation during data entry ensures inputs are valid, prevents duplicate or inconsistent records, and enforces constraints such as a limit of four enchanted book offers per librarian."
  },
  {
    "objectID": "projects/minecraft-trading-database-implementation/index.html#ideas-for-the-future",
    "href": "projects/minecraft-trading-database-implementation/index.html#ideas-for-the-future",
    "title": "Minecraft Villager Trading Database: System Design and Implementation",
    "section": "Ideas for the Future",
    "text": "Ideas for the Future\nIn a follow-up project, I plan to build on this system with several enhancements:\n\nSupport for additional villager professionsâ€”Extend tracking and analysis to professions such as armorers, toolsmiths, and weaponsmiths, where item quality and pricing variability also affect trade value.\nCLI tool for updating existing tradesâ€”Support updating existing trade records to reflect discounted prices from curing zombified villagers.\nETL pipeline version awarenessâ€”Detect when the game version changes and update the enchantments and jobs tables only if the relevant game data has changed.\nVillager scoring modelâ€”Introduce a scoring framework that evaluates villagers based on factors such as trade quality, cost, and relative rarity to help prioritize which to keep or replace.\nInteractive visual dashboardâ€”Provide a graphical interface for exploring trades, viewing villagers, and quickly identifying high-value offers.\n\nTogether, these improvements would transform the system into a more robust tool for managing and optimizing villager trading in Minecraft."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dylan Bretz Jr.",
    "section": "",
    "text": "Hello ðŸ‘‹\nMy name is Dylan :)\nIâ€™m a musician, problem solver, and lifelong learner transitioning to data analytics.\nI bring a unique blend of analytical, creative, and critical thinking to everything I do.\nIâ€™m currently building my portfolio, obtaining certifications, and refining my technical skills.\nI use Python, SQL, R, JavaScript, HTML/CSS, and Markdown, as well as spreadsheets and Tableau.\nIâ€™m currently seeking Data Analyst roles (remote preferred).\nIâ€™m based in Ohio but open to relocating (esp.Â to PNW ðŸŒ²).\nRecently completed:\n\nMinecraft Villager Trading Database: System Design and Implementation\nCyclistic Bike-Share Data Analysis with Python Case Study\nGoogle Data Analytics Professional Certificate\nGoogle Data Analysis with Python Specialization\nGoogle Data-Driven Decision Making Specialization\n\nOutside of work, I enjoy watching movies, listening to music, reading, spending time outdoors, and healthy living.\nThanks for stopping byâ€”more coming soon!\n\n\n\n Back to top"
  }
]